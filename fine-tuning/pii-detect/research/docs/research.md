# Requirements Analysis

The goal is to replace the current GPT-based sensitive data filter with a **self-hosted open-source model** that meets stringent requirements. The system must detect and remove U.S. personal sensitive data (emails, phone numbers, credit card numbers, passport numbers, SSNs, ITINs, driver’s licenses, bank account numbers, etc.) from JSON event payloads in real-time. Key requirements include:

- **Accuracy (Precision & Recall)**: Over **99%** for both precision and recall. The model should catch virtually all sensitive information (minimize false negatives) while avoiding false alarms when data only *appears* sensitive (minimize false positives). This addresses the current shortcomings of the pure rule-based approach.
- **Performance**: Support **100+ QPS** throughput on flagged “suspicious” events (we will only run the model on events first flagged by a lightweight rule-based filter). End-to-end **latency** should ideally be under **1000 ms** per event to enable near real-time filtering.
- **Cost Efficiency**: Eliminate the ongoing cost of calling the OpenAI GPT API for each event. The solution should leverage **open-source models** running on in-house hardware (10 × NVIDIA A10 GPUs) to avoid per-query API fees.
- **Data Compliance & Privacy**: Ensure no sensitive or internal data leaves our environment. Using a self-hosted model means **no external API calls**, mitigating compliance risks that came with sending data to OpenAI.
- **Multilingual Support**: The input data may contain text from various languages (not just English), so the model must handle **multilingual content** effectively. It should correctly identify sensitive info (like phone numbers, credit cards, etc.) even if surrounding text or field names are non-English.
- **Custom Output Format**: The model should output a **structured JSON decision** for each input (with `is_sensitive` true/false and a confidence score). It must reliably follow this output schema for seamless integration into our pipeline.

In summary, we need an *accurate*, *fast*, and *scalable* sensitive-information detector that we control end-to-end. The solution should leverage advanced AI (comparable to GPT’s reasoning) to improve context-sensitive detection, but run cheaply and privately on our own infrastructure.

# Solution Design

## Overall Technical Approach

Given the requirements, the best approach is to fine-tune a **Large Language Model (LLM)** to perform context-aware sensitive data classification. An LLM can understand the *context* around a potential sensitive string (e.g., distinguishing “passport” as a travel document vs. a passport number) far better than rigid regex rules. We will train the model to take an event (or a field of an event) plus the sensitive category as input, and output a JSON verdict (`"is_sensitive": true/false` with a confidence). This closely mirrors the prompt we used with GPT, but now the model will be **specialized** for our task.

Instead of building a classifier from scratch, we leverage an open LLM so we can benefit from its general language understanding and then **instruct-tune** it on our specific task. This approach is feasible because our task can be framed as an instruction (e.g. *“Determine if this content contains a passport number”*) followed by the content. The model will be fine-tuned to *generate* the structured answer.

Using a full LLM (with tens of billions of parameters) for classification might seem heavy, but it ensures high language and context comprehension to hit the 99% accuracy target. In practice, we can apply efficient fine-tuning techniques (like LoRA adapters) to a moderate-sized model to balance accuracy and speed. The model will run in **inference mode** on our GPU servers, listening for flagged events and responding with a decision within milliseconds.

This approach addresses the limitations of the old system:

- **High Recall**: By training on diverse examples of sensitive info, the model can recognize myriad formats and synonyms (e.g. it will learn that "cc#", "Visa", "AMEX", etc., all indicate credit card numbers). This dynamic understanding goes beyond a fixed keyword list.
- **High Precision**: The model uses context to avoid false positives. For example, it can learn that a word like “passport” followed by a 9-digit number likely means a passport ID, but “passport-wallet-tribe-56060137” (a product name with a number) is **not** a passport number. The nuance and context handling of an LLM prevent misclassification in cases that fooled simple regexes.
- **Real-Time**: Once fine-tuned and deployed, the model will run on our GPUs with no external API calls. With optimization, each inference can be made very fast (likely a few tens of milliseconds for a short text), allowing us to meet the sub-second latency budget.

In essence, we are building our own **ChatGPT-like service**, but *specialized* for sensitive data detection. We will use an open-source base model and *post-train* (fine-tune) it on our task. This ensures we maintain full control (for compliance) and can scale usage without incurring per-query fees.

## Dataset Construction

To achieve >99% precision/recall, a robust and representative dataset is crucial. We will construct a **custom training and evaluation dataset** comprising multiple sources:

- **Public Labeled Datasets**: First, we will gather any publicly available data relating to personal identifiable information (PII) detection or data masking. For example, there might be open datasets or competitions for detecting phone numbers, credit card numbers in text, etc. These can provide a baseline of labeled examples (with ground truth whether something is sensitive or not).
- **GPT-Labeled Data**: We will leverage GPT (or another strong model) to label additional data. For instance, we can take a large dump of random web text or sample event logs (with any real sensitive info masked out for compliance) and then **use GPT-4 to identify likely sensitive pieces**. By cross-verifying a few, we can create a sizable synthetic labeled set. The idea is to use GPT’s knowledge to bootstrap our model. For example, feed GPT various JSON snippets or URLs and ask if they contain SSNs, passport numbers, etc., then use those answers as labels for training.
- **GPT-Generated Artful Data**: We will ask GPT to *generate* tricky examples that include or resemble sensitive data. This is important to cover edge cases. For instance, prompt GPT-4 with something like: *“Give me an example of a URL that includes a person’s email and phone number in query parameters”* or *“Create a JSON snippet where it looks like a passport number but isn’t actually one”*. This way we get **hard negatives** (cases that look sensitive but aren’t) and additional positive examples. We will generate examples in multiple languages (e.g. a French ID number in a JSON, a Chinese phone number in text, etc.) to ensure multilingual coverage.
- **No Real Internal Data**: We will **not** include any actual internal user data in the training set (to avoid compliance issues). Instead, any needed patterns from internal data will be recreated synthetically. For example, if we know our web trackers often capture certain URL query patterns, we’ll simulate those.

All data will be labeled with the specific **category** of sensitive info we are checking. In our prompt format, we provide a `"category"` with a name and description (e.g. *"passport"* with a description of U.S. passport numbers). The model can use this to tailor its decision to that category. Our dataset entries will look like:

```json
{
  "content": "http://www.share-fake-passport.com?name=tim&passport=85961047",
  "category": {
    "name": "passport",
    "desc": "United States passport numbers are issued by the U.S. Department of State to U.S. citizens and nationals."
  },
  "label": {
    "is_sensitive": true,
    "confidence": 1.0
  }
}
```

Each entry contains a piece of content and a category, along with the ground-truth label (`is_sensitive`). We might not have a ground-truth numeric `confidence`, but we can assign a surrogate (e.g., 1.0 for definitely sensitive, 0.0 for definitely not) for training. The model will learn to output a confidence between 0 and 1; we can calibrate this using validation data later.

**Dataset Split**: We will combine the above sources into one large dataset. We’ll then split it into training and validation sets. The **validation set** (and a further hold-out test set if needed) will include examples from all sources to ensure the model generalizes. This set will be used to objectively measure precision and recall after training. We aim to get a validation set large enough and diverse enough (covering all categories and languages) to be confident in the >99% P/R metrics.

**Multilingual Considerations**: Because the model should work across languages, our dataset will include variations of field names and context in different languages. For example:

- Passport number contexts in Spanish, French, Chinese, etc. (with translations or local ID equivalent if applicable).
- Emails, phone numbers, credit cards appearing in international contexts (different domain TLDs, different phone formats, etc.).
- We will ensure the base model we choose is multilingual, and augment training with non-English examples so it doesn’t overly bias to English patterns.

## Base Model Selection

We choose the base LLM to fine-tune with care, considering performance, capability, and licensing. Our preference is the **Qwen series of models** (developed by Alibaba) for several reasons:

- **Strong Performance**: Qwen models are state-of-the-art open-source transformers. They have been used to build powerful AI systems and are comparable in quality to Llama 2 and other top models. In fact, Qwen is among the open models that offer huge potential for customization ([superintelligencenews.com](https://superintelligencenews.com/research/tinker-api-thinking-machines-fine-tuning-open-models/#:~:text=The%20motivation%20behind%20Tinker%20stems,with%20these%20models%20often%20requires)). It’s been noted that open models like **LLaMA, Qwen, and Mistral** have made advanced capabilities accessible, but fine-tuning them used to require complex infrastructure ([superintelligencenews.com](https://superintelligencenews.com/research/tinker-api-thinking-machines-fine-tuning-open-models/#:~:text=The%20motivation%20behind%20Tinker%20stems,with%20these%20models%20often%20requires)). By selecting Qwen, we start with a model that has high language understanding and reasoning ability, increasing our chances of achieving >99% accuracy after fine-tuning.
- **Multilingual Capability**: Qwen is known to handle multiple languages (it was developed with English and Chinese in mind, among others). This is important for our use case. While Meta’s LLaMA-2 is also strong in multiple languages, Qwen’s reported prowess in Asian languages could give it an edge for certain non-English data.
- **Model Size vs. Resources**: We have 10 GPUs (NVIDIA A10). We need to choose a model size that can be fine-tuned and served efficiently on this setup. Qwen comes in sizes like 7B and 14B (and larger). A **7B model** is lighter and would give faster inference (better for 100 QPS throughput), but a **14B model** likely has better accuracy and knowledge. Given our high accuracy target, we will likely start with **Qwen-14B** (or an instruct variant of it, if available) as the base. We can apply optimizations like 4-bit quantization or LoRA to make training on A10s feasible. If 14B proves too slow at inference, we can try Qwen-7B as a fallback and see if fine-tuning can close the gap in performance.
- **Open-Weight & License**: Qwen’s weights are open and usable. We must verify the license constraints – Qwen is free to use, with some restrictions for very large-scale commercial deployments (it has a clause about requiring permission if the user base is extremely large). Assuming our use (internal data processing) doesn’t cross those limits, Qwen is a valid choice. Alternately, **LLaMA-2** is another open model we considered (it’s fully open for commercial use). If needed, we could switch to LLaMA-2 13B, but our preference is Qwen due to its performance and multilingual edge. Other models like **Mistral 7B** (highly efficient) or **Baichuan** (Chinese-focused) were also considered, but we prioritize Qwen as a balanced choice.
- **Community & Tools Support**: Qwen is supported by popular fine-tuning frameworks. For instance, the Thinking Machines Lab’s Tinker platform specifically lists support for Qwen models out-of-the-box ([www.thinkingmachines.ai](https://www.thinkingmachines.ai/tinker/#:~:text=Supported%20models)). Also, the open-source **LLaMA-Factory** framework (an efficient fine-tuning toolkit) explicitly supports Qwen-7B and 14B models ([github.com](https://github.com/TingchenFu/LlamaFactory#:~:text=Easy,BLOOM%2C%20Mistral%2C%20Baichuan%2C%20Qwen%2C%20ChatGLM)). This means whichever route we take (managed service or DIY), we’ll have robust support for Qwen.

After selecting the base model (say Qwen-14B), we will utilize an **instruction-tuned variant** if available (for example, Qwen-14B-Chat). Instruction-tuned models are already good at following prompts and producing structured output, which will reduce the amount of fine-tuning needed to get the desired behavior. If an instruct version isn’t directly available, we’ll still be fine-tuning it with our instruction format data, effectively teaching it the task.

## Post-Training Method and Platform

To fine-tune the model, we propose using **parameter-efficient fine-tuning** methods and possibly a **managed training platform** to streamline the process:

- **LoRA (Low-Rank Adaptation)**: Instead of full fine-tuning (updating all 14B parameters, which is slow and memory-heavy), we will use LoRA adapters. LoRA adds a few small trainable weight matrices to the model and leaves the original weights mostly frozen ([superintelligencenews.com](https://superintelligencenews.com/research/tinker-api-thinking-machines-fine-tuning-open-models/#:~:text=Efficiency%20at%20the%20Core%3A%20Why,Tinker%20Uses%20LoRA)). This drastically reduces the GPU memory required and speeds up training. For example, *Thinking Machines Lab’s Tinker platform fine-tunes via LoRA to dramatically reduce compute requirements* ([superintelligencenews.com](https://superintelligencenews.com/research/tinker-api-thinking-machines-fine-tuning-open-models/#:~:text=Efficiency%20at%20the%20Core%3A%20Why,Tinker%20Uses%20LoRA)). LoRA will allow us to fine-tune a big model like Qwen-14B on 10×A10 GPUs without needing extremely specialized infrastructure. It also means the original model’s knowledge is largely preserved, and we’re just *adding* the ability to recognize sensitive data patterns.
- **Supervised Fine-Tuning (SFT)**: Our initial training method will be standard supervised fine-tuning on the labeled dataset. We give the model examples of inputs (content + category) and the correct output (sensitivity label). The model’s objective is to minimize error on these outputs. This will teach it to perform the classification as asked. We will likely fine-tune using a **causal language modeling loss** on the formatted prompt→output pairs (since it’s a generative model producing the answer).
- **Preference Optimization / RLHF**: If simply training on labeled data doesn’t push metrics high enough, we can consider more advanced fine-tuning. For instance, if we had a way to generate comparisons (one model output vs another) and label which is better, we could do **Direct Preference Optimization (DPO)** or Reinforcement Learning from Human Feedback (RLHF) to further refine the model’s judgment on edge cases. However, building a reward model or getting consistent preference data is complex. Given we don’t have an existing human feedback pipeline, we may defer RL-based methods initially. Instead, we can approximate “preference” optimization by heavily penalizing errors on the critical validation examples in training or oversampling tricky cases – essentially a heuristic approach to ensure the model really internalizes the high precision/recall requirement.
- **Managed Fine-Tuning Platform**: We prefer a **one-stop solution** to handle the fine-tuning infrastructure. One attractive option is using **Thinking Machines Lab’s Tinker** platform. Tinker is a managed service that abstracts away the pain of distributed training and infrastructure management ([superintelligencenews.com](https://superintelligencenews.com/research/tinker-api-thinking-machines-fine-tuning-open-models/#:~:text=,technical%20overhead%20and%20compute%20cost)). It allows researchers to fine-tune models on their cluster with minimal fuss: *“It empowers researchers by giving control over algorithms and data while we handle the complexity of distributed training.”* ([indianexpress.com](https://indianexpress.com/article/technology/artificial-intelligence/thinking-machines-debuts-tinker-simplify-fine-tuning-ai-10288853/#:~:text=%E2%80%9CIt%20empowers%20researchers%20and%20hackers,distributed%20training%2C%E2%80%9D%20the%20company%20said)). With Tinker, we can request a training job on, say, a Qwen model with LoRA, and they will handle scaling across GPUs, checkpointing, etc., using their internal GPU cloud. Tinker supports a range of models (including Qwen and LLaMA families) – switching to a larger model is as easy as changing one line of code ([thinkingmachines.ai](https://thinkingmachines.ai/blog/announcing-tinker/#:~:text=Tinker%20lets%20you%20fine,string%20in%20your%20Python%20code)). It also provides low-level APIs if we want to implement custom training loops (even RL or others) ([thinkingmachines.ai](https://thinkingmachines.ai/blog/announcing-tinker/#:~:text=worrying%20about%20managing%20infrastructure,multiple%20training%20runs%2C%20lowering%20costs)), and a “Cookbook” of reference implementations for common methods. Since cost is a concern, note that Tinker uses LoRA to share GPU resources among multiple jobs, making it cost-efficient ([thinkingmachines.ai](https://thinkingmachines.ai/blog/announcing-tinker/#:~:text=infrastructure,multiple%20training%20runs%2C%20lowering%20costs)). (At the time of writing, Tinker was in beta with free access, moving to usage-based pricing ([indianexpress.com](https://indianexpress.com/article/technology/artificial-intelligence/thinking-machines-debuts-tinker-simplify-fine-tuning-ai-10288853/#:~:text=match%20at%20L25%20Qwen,introduced%20in%20the%20coming%20weeks)) – we’d evaluate the pricing vs. running on our own).
- **Open-Source Fine-Tuning Framework**: Alternatively, we can fine-tune on our own GPUs using an open framework like **LLaMA-Factory** or Hugging Face’s Transformer + PEFT libraries. LLaMA-Factory is an **open-source toolkit** for unified fine-tuning of many models (100+ LLMs) and supports Qwen 7B/14B models ([github.com](https://github.com/TingchenFu/LlamaFactory#:~:text=Easy,BLOOM%2C%20Mistral%2C%20Baichuan%2C%20Qwen%2C%20ChatGLM)). It builds on PyTorch and Hugging Face Transformers, making it fairly straightforward to use if we have engineering resources. This approach would avoid any platform costs and keep data fully in-house. We’d need to set up distributed training across our 10 GPUs (possibly using DeepSpeed or FSDP under the hood). This is more work to maintain, but is **“cheap”** in the sense of no service fees – we only pay the electricity and overhead. Given our dataset is not enormous (the QPS of sensitive events is low, so even if we create tens of thousands of examples, that’s manageable), fine-tuning for a few epochs is feasible within a day or two on our hardware.
- **Platform Choice**: Between Tinker and an open-source framework, we will weigh simplicity vs. control. Tinker’s promise is to **“get small or large runs started immediately, without worrying about managing infrastructure.”** ([thinkingmachines.ai](https://thinkingmachines.ai/blog/announcing-tinker/#:~:text=Tinker%20is%20a%20managed%20service,multiple%20training%20runs%2C%20lowering%20costs)) This could save a lot of engineering time, especially if we want to try multiple fine-tuning runs or adjust methods (they even allow custom algorithms via four functions API). On the other hand, using our own setup with LLaMA-Factory might be preferred if we are cautious about data (though our training data is synthetic/public, so less sensitive) or if we want to avoid future costs. Since budget is a concern, we’ll compare the cost of possibly paying for Tinker versus the manpower cost of DIY. Given that Thinking Machines Lab’s mission is to enable folks to fine-tune easily and share their work ([news.smol.ai](https://news.smol.ai/issues/25-10-01-thinky#:~:text=in%20both%20reasoning%20and%20long,go%E2%80%91to%20release%20platform%20for%20new)) ([superintelligencenews.com](https://superintelligencenews.com/research/tinker-api-thinking-machines-fine-tuning-open-models/#:~:text=,technical%20overhead%20and%20compute%20cost)), their platform could be a good investment to jumpstart our solution. We could even start with Tinker for rapid prototyping (it’s likely simpler to get results quickly) and then later transition to our on-premise training if needed.

## Design Summary

Bringing it together, our solution is to fine-tune a **Qwen LLM** (7B or 14B) using LoRA on a carefully curated dataset of sensitive/non-sensitive examples. We will use an **instruction-following format** for the model input/output so that at runtime we can provide a prompt like:

```
Content: "<the URL or text or JSON field>"
Category: "<the category name (e.g. passport)>"
Does this content contain sensitive <category> information? Answer with JSON.
```

And the model will output something like:

```json
{
  "is_sensitive": false,
  "confidence_score": 0.02
}
```

(for a benign example, or `true` with a high score for a sensitive case). The exact prompt format will be refined during training so that the model consistently outputs just the JSON blob and nothing extra.

This model will integrate into our pipeline after the initial rule-based filter. The flow will be: web events → lightweight keyword/regex filter (to flag suspicious) → LLM model for definitive judgment → if model says sensitive, we drop or redact that field.

By using a **frontier model and customizing it to our needs**, we follow the ethos of making advanced AI capabilities accessible and tailored. As Thinking Machines Lab puts it, we are *bridging the gap* by taking cutting-edge open AI and **customizing it to our specific needs** ([thinkingmachines.ai](https://thinkingmachines.ai/blog/announcing-tinker/#:~:text=flexible%20API%20for%20fine,to%20experiment%20with%20models%20by)) – in this case, for data privacy. The combination of a strong base model (with solid “intelligence” foundations) and targeted fine-tuning will give us both **breadth (multilingual, multiple categories)** and **depth (contextual understanding)** needed for the task.

# Execution Steps

To implement this solution, we’ll proceed with the following steps:

**1. Data Gathering and Preparation**

- **Collect Public Data**: Search for any public datasets related to PII or sensitive info detection. For example, look for datasets of leaked email/SSN lists, synthetic identity info, or even general text corpora labeled for personal information. If found, download and incorporate those.
- **Generate Labeled Examples with GPT**: For each sensitive category (Passport, Credit Card, SSN, etc.), prepare a diverse set of examples. Use GPT-4 (via API or internal tool) to label data:
  - Feed it sample text that might contain the sensitive info and ask for yes/no if it contains that category. Use various contexts (some obvious, some tricky) to get a broad labeling. Collect the results in our dataset format.
  - Additionally, prompt GPT-4 to **produce examples**: e.g., “Give 5 examples of a JSON `page_url` containing a credit card number in it” and “5 examples of a `page_url` that looks like it has a credit card but actually doesn’t”. This yields positive and negative samples.
- **Multilingual Augmentation**: Take some of the above examples and translate or modify them into other languages. We might use translation APIs or GPT for this. Ensure that number formats or email formats are realistic for those locales.
- **Aggregate and Label**: Combine all the examples into one dataset. Each example will include the `content` (text/JSON snippet), a `category` with name and description, and a label. Ensure the labels (`is_sensitive`) are correct by spot-checking. For confidence score in training labels, we can set `confidence_score` to 1.0 for things we are sure are sensitive, 0.0 for sure not. For any ambiguous ones, maybe 0.5 – this can help the model learn to use the score. (We might primarily train it to get the classification right; confidence will be secondary and can be calibrated later.)
- **Split Dataset**: Randomly split the dataset into a **training set (e.g. 90%)** and a **validation set (10%)**. However, ensure all categories are represented in the validation set. Possibly, keep a small **hold-out test set** as well for a final unbiased evaluation.

**2. Environment and Model Setup**

- **Choose Model Version**: Download the base model weights for Qwen. For example, get Qwen-14B (and tokenizer, config) from HuggingFace or the official source. If an instruct variant (Qwen-14B-Chat) is available, use that as it’s already instruction-tuned.
- **Infrastructure**: Decide on fine-tuning approach:
  - *Option A: Use Tinker (Managed)* – Sign up or request access to Thinking Machines Lab Tinker. Prepare a training script or use their **Cookbook** for a standard LoRA fine-tuning recipe. The code will specify the model (like `model="Qwen-14B"`), the data loading (point to our dataset, possibly uploaded to cloud storage or passed directly), and training parameters (epochs, batch size, learning rate, LoRA rank, etc.). Then use their API/CLI to launch the training run. Monitor the run via their dashboard or logs. Tinker will automatically handle distributing across GPUs and will use LoRA by default ([thinkingmachines.ai](https://thinkingmachines.ai/blog/announcing-tinker/#:~:text=infrastructure,multiple%20training%20runs%2C%20lowering%20costs)). After training, Tinker will let us download the LoRA adapter weights (and possibly the tokenizer and any logs).
  - *Option B: Use LLaMA-Factory (Self-Hosted)* – Set up our own training environment. This involves installing PyTorch and ensuring it recognizes all 10 GPUs (possibly across machines if needed). Install the **LLaMA-Factory** library (or the Hugging Face Transformers + PEFT). Format our dataset to the expected format (likely a JSONL or CSV with prompt and response columns). Then configure a fine-tuning run. For LLaMA-Factory specifically, we would create a config or use their CLI, specifying model type = Qwen-14B, LoRA parameters (like rank, target modules), and training hyperparameters. Launch the training script, which will use our GPUs (we might use 8 GPUs for training and keep 2 for other tasks). We’d want to use mixed precision (bf16 or fp16) to speed up, and possibly gradient accumulation if memory is a bottleneck. Monitor the training loss and ensure it’s converging.
- **Training Hyperparameters**: In either setup, choose sensible hyperparams. E.g., 2-3 epochs over the data (since our dataset might be a few tens of thousands of examples and we don’t want to overfit), a learning rate around 2e-5 for LoRA, and a batch size that fits in memory (maybe effective batch of a few hundred with accumulation). We’ll also include **early stopping** or at least check validation loss each epoch — since precision/recall are critical, we might evaluate the model on the validation set after each epoch to see if it’s improving or starting to overfit (e.g., if precision goes down, we stop).
- **Fine-Tuning Execution**: Run the fine-tuning. This could take a few hours to a day depending on model size and data size. For example, LoRA fine-tuning a 14B model on a few 10k examples might finish an epoch in a couple of hours on 8 A10 GPUs. Tinker would handle the distribution automatically; with our own setup, we might use PyTorch’s DDP (Distributed Data Parallel) to utilize all GPUs.

**3. Evaluation and Validation**

- After training, use the fine-tuned model to predict on the **validation set**. We’ll feed each validation example (content + category) into the model (likely with the same prompting format we plan to use in production) and collect the outputs.
- Parse the model’s output JSON for each example, and compare `is_sensitive` to the ground truth. Calculate **precision, recall, and F1** on the validation data. This will tell us if we’ve met the >99% target. We should look at these metrics per category as well (to ensure, for instance, it’s not 100% on credit cards but 95% on passport by accident – we need to get all high).
- If the model outputs a confidence score, we can also evaluate how calibrated those are (though it’s less critical than the binary decision). We might adjust how we interpret confidence: for instance, if the model tends to always output 0.99 or 0.01 extremes, that’s fine, but if it outputs middling values, we may set a threshold for what we consider “confidently sensitive”.
- **Error Analysis**: For any mistakes (false negatives or false positives) on validation, do a manual review. Identify why the model got it wrong:
  - If false negative, was it a pattern the model might not have seen (e.g., a novel way to obfuscate a credit card)? We might need to add such examples to training set and retrain.
  - If false positive, was it due to a keyword ambiguity or an output formatting issue? Possibly refine the prompt format or add a counter-example in training.
- If metrics are below target, iterate: possibly do another round of fine-tuning with an expanded dataset focusing on the failure cases (this is a form of **iterative refinement**). Because our pipeline still has the old rule + GPT in place as a fallback during development, we might also run some live traffic through the new model (in shadow mode) to gather any misses and incorporate those before full deployment.

**4. Model Optimization for Deployment**

- Once we have a model that performs well, we prepare it for efficient inference. If we used LoRA, we have two options:
  - Merge the LoRA weights into the base model to get a standalone model checkpoint (this makes inference simpler with one model file, but the model remains the same size in memory).
  - Keep the base and LoRA separate and use a library that supports loading a base model with LoRA adapters (useful if we want to potentially toggle the LoRA on/off or update it easily). Most inference frameworks can apply LoRA on the fly with negligible overhead.
- **Quantization**: We will likely use 8-bit or 4-bit quantization for the model in inference to reduce memory and increase speed. Tools like Hugging Face’s `transformers` (with bitsandbytes) or NVIDIA’s TensorRT can compress the model. For example, **Qwen-14B 4-bit** quantized can significantly cut memory usage, allowing us to run multiple model instances or batch queries. We must ensure quantization doesn’t degrade the model’s accuracy; we’ll test the quantized model on the validation set to verify its outputs remain correct (they usually do for classification tasks if quantization is done properly).
- **Inference Framework**: Set up a high-performance inference server. We can use **vLLM** or HuggingFace’s **Text Generation Inference (TGI)** server for optimized serving. These systems support features like dynamic batching (grouping multiple requests to utilize GPU efficiently) and caching. For our scenario, we might set a small batch size (since each input is short) and let the server batch incoming requests up to that size within a few milliseconds window. This way, if 100 QPS requests come in, the server might handle them in, say, 20 batches of 5 at a time, greatly boosting throughput.
- **Latency Tuning**: Ensure the prompt is minimal to save time. Unlike the long GPT prompt we used (with detailed instructions), our fine-tuned model can have a very short prompt format, because it already knows the task. For example, the input to the model could be just: `"[Category: Passport] Content: ..."` and that’s it – the model was trained to infer the task from this format. This reduces token processing per request. Also, limit the output tokens: since we only need a couple lines of JSON, we might put a max generation length of, say, 50 tokens. With these settings, a single forward pass might take, hypothetically, 50ms on one A10 for a 14B model (could be more or less – we’ll benchmark).
- **Scaling to 100 QPS**: If one model instance (on one GPU) can handle, for example, 10 QPS with low latency, we can scale up by running multiple instances on multiple GPUs. With 10 A10 GPUs, in the worst case, we allocate 1 GPU per model instance and achieve 10× throughput of a single GPU. Possibly we can run two lighter instances per GPU if memory allows (especially with quantization). We will also take advantage of concurrency – these frameworks allow multiple requests to be processed in parallel on a single GPU if the model isn’t fully saturated. The exact setup will be tuned based on profiling.
- **Integration**: Wrap the model server behind an API that our data pipeline can call (or even embed it in the streaming pipeline if using something like Triton Inference Server). Essentially, replace the OpenAI API call with a call to our internal service. We should also have a fallback or circuit-breaker – e.g., if the model service is ever down or overloaded, we might temporarily revert to the old plan (or at least fail-safe by dropping any suspicious data by default).

**5. Deployment & Monitoring**

- Deploy the model service in our production environment. For reliability, perhaps use two instances (two sets of GPUs) behind a load balancer for redundancy.
- **Monitor** the performance: log the model’s decisions alongside what the rule-based filter found, to ensure it’s accurately catching everything. Also monitor latency (each request’s time) and throughput, ensuring we meet the <1000ms target consistently.
- **Ongoing Evaluation**: Even after deployment, keep an eye on precision/recall by doing periodic audits. For instance, randomly sample events that the model marked as non-sensitive but had some number, and double-check they truly weren’t sensitive (to catch any new false negatives). Maintain a feedback loop: if any issues are found, collect those cases and add to the training set for the next model update.
- As a precaution initially, we might run the new model in shadow mode (just logging outcomes) for a short period while still using the old GPT to actually decide. This way we compare decisions and ensure the new model is on par or better, before fully switching over.

By following these steps, we’ll go from data collection to a fine-tuned model and have it smoothly integrated, replacing the expensive GPT calls with our in-house solution.

# Potential Challenges & Solutions

Implementing this solution won’t be without challenges. Here are some potential issues and how we plan to address them:

- **Dataset Quality and Coverage**: Since we are largely synthetically generating data, there’s a risk of not covering some real-world pattern of sensitive info. For example, maybe our data didn’t include a certain way a phone number can appear. *Mitigation*: Iteratively improve the dataset. After initial deployment, any time the model misses something (or flags something weird), treat it as a new example to add. We can schedule periodic re-training or fine-tune updates to incorporate newly observed patterns. Also, leverage the rule-based system to flag anything with unknown patterns; if it wasn’t caught by the model, that pattern can be included in the next training round.
- **False Positives**: The model might initially be slightly over-cautious and flag benign data as sensitive (e.g., the word “social” followed by some number might confuse it as SSN). *Mitigation*: Emphasize context in training. Include many negative examples where similar keywords appear in non-sensitive contexts. Also, we can post-process the model’s `confidence_score`. For instance, if the model isn’t very sure (score ~0.5) and it’s labeling something sensitive, we might choose to not drop that data immediately but send it for secondary review or apply an extra strict regex check. Setting a high threshold for the model’s decision (like only treat `is_sensitive=true` if confidence > 0.8) can improve precision, though at a slight cost to recall. We’ll calibrate this threshold based on validation results.
- **Multilingual Edge Cases**: Languages with different scripts (Chinese, Arabic) might format numbers differently or have different ID types. The base model (Qwen) should be able to handle these languages, but our fine-tuning data might be sparse in them. *Mitigation*: Use translation or multilingual data generation techniques. If needed, engage native speakers or knowledge to craft a few examples of, say, how national ID numbers look in those languages. We can always expand the training set over time for more languages. The open-source model approach means we’re not limited – we can continuously refine it as we encounter new languages or formats.
- **License and Compliance**: While we avoid sending data out, we must ensure our use of the model complies with its license. Qwen’s license, for instance, has some restrictions. *Solution*: Verify with legal or with Alibaba if necessary. If Qwen’s license is an issue for our company (say we are above the user count threshold), we might negotiate a license or switch to an alternative like LLaMA-2 (which is fully open for commercial use). This is a one-time consideration when choosing the base model.
- **Model Hallucination or Format Errors**: LLMs sometimes produce answers that don’t exactly match the expected format (e.g., extra text around the JSON). We have to be strict since our parser expects a JSON. *Mitigation*: Our fine-tuning prompts specifically instruct the model to output JSON and we provide lots of examples, so it should learn to do so reliably. We can also put a simple regex-based validator on the output – if it’s not valid JSON, possibly try to clean it or in worst case, default to removing the field (fail-safe). However, with fine-tuning, we expect near 100% compliance on the output format, as the model will treat it as a learned behavior.
- **Scaling & Throughput**: Achieving 100 QPS with a 14B model might be challenging if not optimized. We have to consider that each event’s input prompt is relatively short (which helps). *Solution*: We will optimize at multiple levels – quantize the model to reduce computation, use efficient batching, and possibly use multiple GPUs in parallel. If a single request takes ~50ms on one GPU, one GPU could handle ~20 QPS; five such GPUs could handle 100 QPS. We have 10 GPUs, so we have headroom. If needed, we can also decrease the model size (e.g., try Qwen-7B) to see if it still meets accuracy – the smaller model will be much faster and could possibly handle 100 QPS on fewer GPUs. Another avenue is using a distillation: train a smaller model (like a 2-3B parameter model or even a classifier neural network) on the outputs of the big model to serve as a lightweight runtime filter. That could be a future optimization if needed.
- **Integration Complexity**: Running an LLM service internally introduces new system components (GPU servers, model serving frameworks) which are more complex than the previous simple rule service. *Mitigation*: Keep the design as simple as possible. We can containerize the inference server and deploy it as a microservice. Leverage proven libraries (like the TGI server or vLLM) so we don’t write serving code from scratch. Also, invest in good monitoring and logging for this service given its critical role.
- **Keeping Both Precision and Recall High**: Often there’s a trade-off – if we push recall to 99%, precision might dip. We have to be very careful in training to balance this. *Solution*: Possibly treat this as an optimization problem where false negatives are absolutely unacceptable (so recall first), and then tune to reduce false positives. In practice, since we have the ability to generate abundant training data, we can try to basically force the model to learn almost deterministic patterns for known sensitive formats, while also learning context for ambiguous ones. If needed, we can adjust the **loss function** during fine-tuning to weight errors on positive vs negative examples differently (e.g., give a higher penalty for missing a sensitive instance than for wrongly flagging a non-sensitive instance). This way, the model is trained to favor recall slightly – then we can use the confidence score and thresholding to reel in precision.

By anticipating these challenges, we can incorporate solutions early (for example, adding those extra negative examples, or planning for quantization from the start). The flexibility of having our own model is that we can iteratively improve it and **measure what truly matters** – the real-world performance on our data ([indianexpress.com](https://indianexpress.com/article/technology/artificial-intelligence/thinking-machines-debuts-tinker-simplify-fine-tuning-ai-10288853/lite/#:~:text=Thinking%20Machines%20debuts%20Tinker%2C%20a,a%20wide%20range%20of%20large)). We will focus on the objective of correctly filtering sensitive information rather than just optimizing abstract metrics, aligning with best practices in applied AI development.

# Further Optimization and Future Directions

Once the initial system is up and running with the open-source model, there are several ways we can further improve and future-proof the solution:

- **Continual Learning**: The landscape of web data is always evolving. New types of sensitive data (or new ways of encoding them) might appear. We should establish a pipeline for continual learning – for instance, **monthly model updates** incorporating the latest flagged data. With our own infrastructure, we can fine-tune the model further on new examples easily. Using a platform like Tinker or our own setup, spinning up a fine-tuning job every so often with additional data can keep the model up-to-date. We just have to ensure we don’t drift (always validate on a stable test set to ensure no regression).
- **Feedback Loop and Human-in-the-Loop**: To reach and maintain >99% accuracy, we might integrate a human-in-the-loop for the rare uncertain cases. For example, if the model’s confidence is very low or very borderline on an event, that event could be flagged for a privacy analyst to review (depending on latency allowances, this might be offline). The feedback from that analyst (saying it was sensitive or not) can then be fed back as a training example. This kind of **active learning** can catch the most challenging cases. It also aligns with an emphasis on *human-AI collaboration* – using AI to handle the bulk automatically, but involving humans for oversight on the trickiest parts ([theoutpost.ai](https://theoutpost.ai/news-story/thinking-machines-lab-unveils-tinker-a-game-changing-ai-model-fine-tuning-tool-20612/#:~:text=was%20already%20in%20use%20across,include%20teams%20from%2C%20yes%2C%20Berkeley)).
- **Enhanced Model Techniques**: We did basic fine-tuning, but we could explore advanced training techniques:
  - **Direct Preference Optimization (DPO)** or **Reinforcement Learning**: If we get a stream of model decisions and outcomes, we could train a reward model that captures “bad outcome” (missed sensitive info or false alarm) and use RL (like PPO – Proximal Policy Optimization) to fine-tune the policy (the model) to avoid those. Essentially, we can encode our precision/recall balance into a reward: heavily penalize misses, slightly penalize false hits, and optimize. This is complex and requires careful setup (and careful not to wreck the model’s base abilities), but it could squeeze out extra performance. Thinking Machines Lab suggests that a combination of proactive research and **careful real-world testing is key to AI safety** ([indianexpress.com](https://indianexpress.com/article/technology/artificial-intelligence/thinking-machines-debuts-tinker-simplify-fine-tuning-ai/#:~:text=Thinking%20Machines%20debuts%20Tinker%2C%20a,A22B.%20Image%3A%20TinkerResearchers)) – in our context, that means trying these advanced fine-tuning methods in a controlled way and testing thoroughly.
  - **Multi-Task Learning**: We could fine-tune the model on multiple tasks at once. For instance, maybe we also want the model to categorize what type of sensitive info it found, or to extract the actual value (for future masking). We could multi-task the training to have the model not just say sensitive or not, but also output an explanation or category. This might enrich its understanding. Even if we don’t expose that in production, it could be useful for internal logs (e.g., “removed credit card number 4111-xxxx from URL”).
  - **Ensemble or Two-Step Models**: We might use a small secondary model to double-check the primary model. For example, a lightweight regex-based classifier or an ML model could quickly say “there’s a 16-digit number present and the word card, so high chance of credit card” – if our LLM said non-sensitive in such a case, we might have the ensemble override or at least log a warning. Conversely, if the LLM flags something with low confidence, a secondary check might decide to ignore it. Ensemble approaches can sometimes boost reliability, though they add complexity. We can simulate an ensemble by simply leaving the initial rules in place as a sanity check (maybe not to make final decision, but to alert us of any obvious contradiction with the model’s output).
- **Efficiency Improvements**: To further reduce latency and resource usage:
  - We could try **distilling** the fine-tuned model into a smaller model. For example, after training Qwen-14B, we could use its outputs to train a 3B or 7B model on the same task (or even a specialized classifier model). If successful, that smaller model might achieve nearly as good accuracy at a fraction of the inference cost, allowing much higher QPS or lower latency. This is a common technique to deploy smaller models after using a large one as a teacher.
  - Investigate model compilation techniques like NVIDIA’s TensorRT or ONNX Runtime for transformers to speed up inference. These can give a constant speedup once the model is exported and optimized, especially on NVIDIA GPUs.
  - Keep an eye on newer open-source models. The open AI space is moving fast – for instance, models like **Mistral 7B** have shown excellent performance comparable to larger models. If a newer 7B or 13B model comes out that offers equal performance to Qwen-14B, we might switch to reduce latency. Since our fine-tuning pipeline is in place, trying a new base model would be as simple as plugging it in and transferring our dataset (possibly even using our fine-tuned model’s outputs to quick-start it).
- **Broader Scope**: Currently we apply the model only to flagged events (to save computation). In the future, if our system or hardware grows, we could consider using the model on **all events**, removing even the initial keyword filter. This would simplify the architecture (one stage instead of two). To do this efficiently, we’d want an extremely optimized model (maybe after some distillation or using a smaller model as mentioned). It’s an eventual possibility, especially as hardware gets cheaper or if we get a model that’s nearly as fast as regex with comparable accuracy.
- **Monitoring and Model Governance**: As this model becomes a core part of our data handling, we should implement good monitoring. This includes drift detection (if the distribution of inputs changes significantly, our model might start degrading – we’d detect that via a drop in confidence or a change in the pattern of outputs). Also, maintain transparency on what the model is doing – log decisions especially when dropping data, so if anything ever needs audit (for compliance, we can show why a certain piece of data was removed – e.g., model thought it was a SSN). Internally, we might even keep a counter of how many sensitive items of each type we filter out (useful metrics for the business, e.g., “we blocked 500 credit card numbers this week”).
- **Security and Misuse Prevention**: Although this model runs internally, one could consider adversarial inputs – e.g., what if someone intentionally tries to sneak sensitive info past it by obfuscation? We should test some adversarial scenarios (like numbers with zero-width spaces, or base64-encoded data). While our primary directive is to not miss things, it’s worth considering how someone might try to game the filter. We can then enhance the model or pre-processing (normalize inputs, remove weird characters) to handle that. This ties into broader AI safety and alignment thinking: *“preventing misuse of our released models while maximizing users’ freedom”* ([indianexpress.com](https://indianexpress.com/article/technology/artificial-intelligence/thinking-machines-debuts-tinker-simplify-fine-tuning-ai/#:~:text=Thinking%20Machines%20debuts%20Tinker%2C%20a,A22B.%20Image%3A%20TinkerResearchers)) – in our case, misuse means slipping through sensitive data, and user freedom is not too relevant except we don’t want to falsely censor normal data. So balancing that is key.

In conclusion, by replacing GPT with a fine-tuned open-source model, we gain independence and control. This lets us **continuously refine the model** to our needs, incorporate community best practices, and experiment with cutting-edge fine-tuning methods in the future. The combination of a strong base model like Qwen and accessible tools (like Tinker’s managed fine-tuning or open frameworks) empowers us to build an AI system tailored to our unique requirements. As the Thinking Machines Lab philosophy suggests, this approach of sharing knowledge and tools, and customizing AI for specific goals, is exactly how we bridge the gap between advanced AI capabilities and real-world needs ([thinkingmachines.ai](https://thinkingmachines.ai/blog/announcing-tinker/#:~:text=flexible%20API%20for%20fine,to%20experiment%20with%20models%20by)) ([news.smol.ai](https://news.smol.ai/issues/25-10-01-thinky#:~:text=in%20both%20reasoning%20and%20long,go%E2%80%91to%20release%20platform%20for%20new)). Our sensitive data filter will become smarter, faster, and safer over time, ultimately helping protect user privacy without sacrificing performance or cost-efficiency.

---
Learn more:

1. [Inside Tinker: How Thinking Machines Lab Is Reinventing Fine-Tuning for the Open AI Era - Superintelligence News - Artificial Intelligence News](https://superintelligencenews.com/research/tinker-api-thinking-machines-fine-tuning-open-models/#:~:text=The%20motivation%20behind%20Tinker%20stems,with%20these%20models%20often%20requires)
2. [Tinker - Thinking Machines Lab](https://www.thinkingmachines.ai/tinker/#:~:text=Supported%20models)
3. [GitHub - TingchenFu/LlamaFactory: Easy-to-use LLM fine-tuning framework (LLaMA, BLOOM, Mistral, Baichuan, Qwen, ChatGLM)](https://github.com/TingchenFu/LlamaFactory#:~:text=Easy,BLOOM%2C%20Mistral%2C%20Baichuan%2C%20Qwen%2C%20ChatGLM)
4. [Inside Tinker: How Thinking Machines Lab Is Reinventing Fine-Tuning for the Open AI Era - Superintelligence News - Artificial Intelligence News](https://superintelligencenews.com/research/tinker-api-thinking-machines-fine-tuning-open-models/#:~:text=Efficiency%20at%20the%20Core%3A%20Why,Tinker%20Uses%20LoRA)
5. [Inside Tinker: How Thinking Machines Lab Is Reinventing Fine-Tuning for the Open AI Era - Superintelligence News - Artificial Intelligence News](https://superintelligencenews.com/research/tinker-api-thinking-machines-fine-tuning-open-models/#:~:text=,technical%20overhead%20and%20compute%20cost)
6. [Thinking Machines debuts Tinker, a developer tool to simplify fine-tuning of AI models | Technology News - The Indian Express](https://indianexpress.com/article/technology/artificial-intelligence/thinking-machines-debuts-tinker-simplify-fine-tuning-ai-10288853/#:~:text=%E2%80%9CIt%20empowers%20researchers%20and%20hackers,distributed%20training%2C%E2%80%9D%20the%20company%20said)
7. [Announcing Tinker - Thinking Machines Lab](https://thinkingmachines.ai/blog/announcing-tinker/#:~:text=Tinker%20lets%20you%20fine,string%20in%20your%20Python%20code)
8. [Announcing Tinker - Thinking Machines Lab](https://thinkingmachines.ai/blog/announcing-tinker/#:~:text=worrying%20about%20managing%20infrastructure,multiple%20training%20runs%2C%20lowering%20costs)
9. [Announcing Tinker - Thinking Machines Lab](https://thinkingmachines.ai/blog/announcing-tinker/#:~:text=infrastructure,multiple%20training%20runs%2C%20lowering%20costs)
10. [Thinking Machines debuts Tinker, a developer tool to simplify fine-tuning of AI models | Technology News - The Indian Express](https://indianexpress.com/article/technology/artificial-intelligence/thinking-machines-debuts-tinker-simplify-fine-tuning-ai-10288853/#:~:text=match%20at%20L25%20Qwen,introduced%20in%20the%20coming%20weeks)
11. [Announcing Tinker - Thinking Machines Lab](https://thinkingmachines.ai/blog/announcing-tinker/#:~:text=Tinker%20is%20a%20managed%20service,multiple%20training%20runs%2C%20lowering%20costs)
12. [Thinking Machines' Tinker: LoRA based LLM fine-tuning API | AINews](https://news.smol.ai/issues/25-10-01-thinky#:~:text=in%20both%20reasoning%20and%20long,go%E2%80%91to%20release%20platform%20for%20new)
13. [Announcing Tinker - Thinking Machines Lab](https://thinkingmachines.ai/blog/announcing-tinker/#:~:text=flexible%20API%20for%20fine,to%20experiment%20with%20models%20by)
14. [Thinking Machines debuts Tinker, a developer tool to simplify fine-tuning of AI models | Technology News - The Indian Express](https://indianexpress.com/article/technology/artificial-intelligence/thinking-machines-debuts-tinker-simplify-fine-tuning-ai-10288853/lite/#:~:text=Thinking%20Machines%20debuts%20Tinker%2C%20a,a%20wide%20range%20of%20large)
15. [Thinking Machines Lab Unveils Tinker: An API for AI Model Fine-Tuning](https://theoutpost.ai/news-story/thinking-machines-lab-unveils-tinker-a-game-changing-ai-model-fine-tuning-tool-20612/#:~:text=was%20already%20in%20use%20across,include%20teams%20from%2C%20yes%2C%20Berkeley)
16. [Thinking Machines debuts Tinker, a developer tool to simplify fine-tuning of AI models | Technology News - The Indian Express](https://indianexpress.com/article/technology/artificial-intelligence/thinking-machines-debuts-tinker-simplify-fine-tuning-ai/#:~:text=Thinking%20Machines%20debuts%20Tinker%2C%20a,A22B.%20Image%3A%20TinkerResearchers)
